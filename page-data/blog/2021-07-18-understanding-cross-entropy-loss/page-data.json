{"componentChunkName":"component---src-components-jupyter-blog-layout-js","path":"/blog/2021-07-18-understanding-cross-entropy-loss/","result":{"data":{"jupyterNotebook":{"id":"e722cd17-703d-538d-a3e5-32b131142ab9 >>> JupyterNotebook","html":"<div class=\"notebook-render\"><div class=\"sc-ifAKCX zMCZx\"><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><h1>Understanding Cross-Entropy Loss</h1>\n<p><strong>Recall:</strong> a loss function compares the predictions of a model with the correct labels to tell us how well the model is doing, and to help find out how we can update the model&#x27;s parameters to improve its performance (using gradient descent).</p>\n<p><strong>Cross-entropy</strong> is a loss function we can use to train a model when the output is one of several classes. For example, we have 10 classes to choose from when trying to predict which number an image of a single digit represents.</p>\n<p><img src=\"/prd8.png\"/></p>\n<p><em>In this image, the number we are trying to predict belongs to the class representing the digit 8.</em></p>\n<p>To use the cross-entropy loss, we need to have as many outputs from our model as the number of possible classes. The cross-entropy loss then enables us to train the model such that the value of the output corresponding to the correct prediction is high, and for the other outputs it is low.</p>\n<p>The first step of using the cross-entropy loss function is passing the raw outputs of the model through a <strong>softmax layer</strong>. A softmax layer squishes all the outputs of the model between 0 and 1. It also ensures that all these values combined add up to 1.</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span class=\"token\" style=\"color:#0000ff\">import</span><span> torch\n</span>\n<span>t </span><span class=\"token\" style=\"color:#393A34\">=</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>tensor</span><span class=\"token\" style=\"color:#393A34\">(</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#393A34\">-</span><span class=\"token\" style=\"color:#36acaa\">9</span><span class=\"token\" style=\"color:#393A34\">,</span><span> </span><span class=\"token\" style=\"color:#36acaa\">5</span><span class=\"token\" style=\"color:#393A34\">,</span><span> </span><span class=\"token\" style=\"color:#36acaa\">10</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">,</span><span> dtype</span><span class=\"token\" style=\"color:#393A34\">=</span><span>torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>double</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span><span>torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>softmax</span><span class=\"token\" style=\"color:#393A34\">(</span><span>t</span><span class=\"token\" style=\"color:#393A34\">,</span><span> dim</span><span class=\"token\" style=\"color:#393A34\">=</span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">)</span></code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([[5.5653e-09, 6.6929e-03, 9.9331e-01]], dtype=torch.float64)</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>Mathematically, each of the values above is calculated as follows:</p>\n<p><img src=\"/sm-eqn.png\"/></p>\n<p>We can create a function to calculate the softmax on our own as follows:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span class=\"token\" style=\"color:#0000ff\">def</span><span> </span><span class=\"token\" style=\"color:#393A34\">softmax</span><span class=\"token\" style=\"color:#393A34\">(</span><span>x</span><span class=\"token\" style=\"color:#393A34\">)</span><span class=\"token\" style=\"color:#393A34\">:</span><span>\n</span><span>  </span><span class=\"token\" style=\"color:#0000ff\">return</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>exp</span><span class=\"token\" style=\"color:#393A34\">(</span><span>x</span><span class=\"token\" style=\"color:#393A34\">)</span><span> </span><span class=\"token\" style=\"color:#393A34\">/</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>exp</span><span class=\"token\" style=\"color:#393A34\">(</span><span>x</span><span class=\"token\" style=\"color:#393A34\">)</span><span class=\"token\" style=\"color:#393A34\">.</span><span class=\"token builtin\">sum</span><span class=\"token\" style=\"color:#393A34\">(</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>\n<span>softmax</span><span class=\"token\" style=\"color:#393A34\">(</span><span>t</span><span class=\"token\" style=\"color:#393A34\">)</span></code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([[5.5653e-09, 6.6929e-03, 9.9331e-01]], dtype=torch.float64)</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>Each value can be interpreted as the confidence with which the model predicts the corresponding output as the correct class.</p>\n<p>Since the exponential function is used in the softmax layer, any raw output from the model that is slightly higher than another will be amplified by the softmax layer.</p>\n<p><img src=\"/exp.png\"/></p>\n<p><em>The exponential function amplifies even small differences in input values</em></p>\n<p>This goes to show that the softmax layer tries its best to pick one single value as the correct model output. As a result, this layer works really well when trying to train a classifier that has to pick one correct category.</p>\n<p>On the other hand, if you want a model not to pick a class just because it has just a slightly higher output value, it is advisable to use the sigmoid function with each individual output.</p>\n<p>After the softmax layer, the second part of the cross-entropy loss is the <strong>log likelihood</strong>.</p>\n<p>By using the softmax layer, we have condensed the value of each output between 0 and 1. This greatly reduces the sensitivity of the model confidence. For example, a prediction of 0.999 can be interpreted as being 10 times more confident than a prediction of 0.99. However, on a softmax scale, the difference between the two values is minuscule - a mere 0.009!</p>\n<p>By taking the log of the values, we can amplify even such small differences.</p>\n<p>For example,</p>\n<div class=\"math\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><semantics><mrow><mfrac><mn>0.999</mn><mn>0.99</mn></mfrac><mo>×</mo><mn>100</mn><mi mathvariant=\"normal\">%</mi><mo>=</mo><mtext>approx. 0.9% difference in confidence</mtext></mrow><annotation encoding=\"application/x-tex\">\\frac{0.999}{0.99} \\times 100\\% = \\text{approx. 0.9\\% difference in confidence}</annotation></semantics></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.00744em;vertical-align:-0.686em\"></span><span class=\"mord\"><span class=\"mopen,nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t,vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em\"><span style=\"top:-2.314em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\">9</span></span></span><span style=\"top:-3.23em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em\"></span></span><span style=\"top:-3.677em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\">9</span><span class=\"mord\">9</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em\"><span></span></span></span></span></span><span class=\"mclose,nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">%</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.94444em;vertical-align:-0.19444em\"></span><span class=\"mord,text\"><span class=\"mord\">approx. 0.9% difference in confidence</span></span></span></span></span></span></div>\n<p>However,</p>\n<div class=\"math\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><semantics><mrow><mfrac><mrow><mi>log</mi><mo>⁡</mo><mn>0.999</mn></mrow><mrow><mi>log</mi><mo>⁡</mo><mn>0.99</mn></mrow></mfrac><mo>×</mo><mn>100</mn><mi mathvariant=\"normal\">%</mi><mo>=</mo><mtext>approx. 10% difference in confidence</mtext></mrow><annotation encoding=\"application/x-tex\">\\frac{\\log{0.999}}{\\log{0.99}} \\times 100\\% = \\text{approx. 10\\% difference in confidence}</annotation></semantics></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em\"></span><span class=\"mord\"><span class=\"mopen,nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t,vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714399999999998em\"><span style=\"top:-2.314em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"mord\"><span class=\"mop\">lo<span style=\"margin-right:0.01389em\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\">9</span></span></span></span><span style=\"top:-3.23em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em\"></span></span><span style=\"top:-3.677em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"mord\"><span class=\"mop\">lo<span style=\"margin-right:0.01389em\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\">9</span><span class=\"mord\">9</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804400000000001em\"><span></span></span></span></span></span><span class=\"mclose,nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">%</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.94444em;vertical-align:-0.19444em\"></span><span class=\"mord,text\"><span class=\"mord\">approx. 10% difference in confidence</span></span></span></span></span></span></div>\n<p>On a log scale, number close to 0 are pushed towards negative infinity and numbers close to 1 are pushed towards 0.</p>\n<p><img src=\"/lgx.png\"/></p>\n<p>Let us now consider the model output value corresponding to the correct class. If we maximize this value, then all the other values will be automatically minimized (since all values have to add up to 1 because of the softmax layer).</p>\n<p>If the output of the model corresponding to the correct class is close to 1, its log value will be close to 0. However, if the model output is close to 0, its log value will be highly negative (close to negative infinity).</p>\n<p>We need the value of the loss function to be high when the prediction is incorrect, and low when the prediction is correct. We can have this with the log values if we drop the negative sign (or equivalently, multiply the value by -1). Then, when the model output is close to 0 for the correct class (incorrect prediction), the negative log value will be extremely high and when the model output is close to 1 for the correct class (correct prediction), the negative log value will be close to 0.</p>\n<p><img src=\"/neg-lgx.png\"/></p>\n<p>We can then use this as a loss function to maximize the output of the model corresponding to the correct class. Like we saw before, this will automatically minimize the outputs of the other classes because of the softmax function.</p>\n<p>This combination of softmax and log-likelihood is the cross-entropy loss.</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>To see how this all works with PyTorch, let us assume we have 3 data points that can belong to one of 5 classes.</p>\n<p>Assume our model produces the following output for these 3 data points:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>model_output </span><span class=\"token\" style=\"color:#393A34\">=</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>randn</span><span class=\"token\" style=\"color:#393A34\">(</span><span class=\"token\" style=\"color:#393A34\">(</span><span class=\"token\" style=\"color:#36acaa\">3</span><span class=\"token\" style=\"color:#393A34\">,</span><span> </span><span class=\"token\" style=\"color:#36acaa\">5</span><span class=\"token\" style=\"color:#393A34\">)</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>model_output\n</code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([[-0.7514,  1.2616, -0.4232,  1.3868,  1.2298],\n        [ 1.5341, -0.4240,  0.0112, -0.5188,  0.1129],\n        [-2.6143,  0.1532,  0.0868, -0.8231,  0.7698]])</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>Let us also assume that the correct classes for these data points are as follows:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>targets </span><span class=\"token\" style=\"color:#393A34\">=</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>tensor</span><span class=\"token\" style=\"color:#393A34\">(</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">3</span><span class=\"token\" style=\"color:#393A34\">,</span><span> </span><span class=\"token\" style=\"color:#36acaa\">0</span><span class=\"token\" style=\"color:#393A34\">,</span><span> </span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>targets\n</code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([3, 0, 1])</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>We first pass these outputs through a softmax layer:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>sm </span><span class=\"token\" style=\"color:#393A34\">=</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>softmax</span><span class=\"token\" style=\"color:#393A34\">(</span><span>model_output</span><span class=\"token\" style=\"color:#393A34\">,</span><span> dim </span><span class=\"token\" style=\"color:#393A34\">=</span><span> </span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>sm\n</code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([[0.0390, 0.2923, 0.0542, 0.3313, 0.2832],\n        [0.5784, 0.0816, 0.1261, 0.0742, 0.1396],\n        [0.0149, 0.2365, 0.2213, 0.0891, 0.4382]])</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>As expected, all values have been squished between 0 and 1.</p>\n<p>We can also confirm that for each data point, the values sum up to 1:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>sm</span><span class=\"token\" style=\"color:#393A34\">.</span><span class=\"token builtin\">sum</span><span class=\"token\" style=\"color:#393A34\">(</span><span>dim</span><span class=\"token\" style=\"color:#393A34\">=</span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">)</span></code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([1.0000, 1.0000, 1.0000])</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>Next, we take the log of these values:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>lg </span><span class=\"token\" style=\"color:#393A34\">=</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>log</span><span class=\"token\" style=\"color:#393A34\">(</span><span>sm</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>lg\n</code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor([[-3.2429, -1.2300, -2.9147, -1.1048, -1.2618],\n        [-0.5476, -2.5056, -2.0705, -2.6004, -1.9687],\n        [-4.2092, -1.4417, -1.5081, -2.4180, -0.8251]])</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>We can then use <code>nll_loss</code> (i.e. Negative Log Likelihood) that will find the mean of the values corresponding to the correct class. This function will also multiply the values by -1 for us before doing so.</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span class=\"token\" style=\"color:#0000ff\">import</span><span> torch</span><span class=\"token\" style=\"color:#393A34\">.</span><span>nn</span><span class=\"token\" style=\"color:#393A34\">.</span><span>functional </span><span class=\"token\" style=\"color:#0000ff\">as</span><span> F\n</span>\n<span>loss </span><span class=\"token\" style=\"color:#393A34\">=</span><span> F</span><span class=\"token\" style=\"color:#393A34\">.</span><span>nll_loss</span><span class=\"token\" style=\"color:#393A34\">(</span><span>lg</span><span class=\"token\" style=\"color:#393A34\">,</span><span> targets</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>loss\n</code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor(1.0314)</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>We can manually verify this for the 3 data points:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span class=\"token\" style=\"color:#393A34\">-</span><span class=\"token\" style=\"color:#36acaa\">1</span><span> </span><span class=\"token\" style=\"color:#393A34\">*</span><span> </span><span class=\"token\" style=\"color:#393A34\">(</span><span>lg</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">0</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">[</span><span>targets</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">0</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">]</span><span> </span><span class=\"token\" style=\"color:#393A34\">+</span><span> lg</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">[</span><span>targets</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">]</span><span> </span><span class=\"token\" style=\"color:#393A34\">+</span><span> lg</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">2</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">[</span><span>targets</span><span class=\"token\" style=\"color:#393A34\">[</span><span class=\"token\" style=\"color:#36acaa\">2</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">]</span><span class=\"token\" style=\"color:#393A34\">)</span><span> </span><span class=\"token\" style=\"color:#393A34\">/</span><span> </span><span class=\"token\" style=\"color:#36acaa\">3</span></code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor(1.0314)</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>Note that the <code>nll_loss</code> function assumes that the log has been taken before the values are passed to the function.</p>\n<p>PyTorch has a <code>log_softmax</code> function that combines softmax with log in one step. We can use that function to achieve the same result as follows:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>lsm </span><span class=\"token\" style=\"color:#393A34\">=</span><span> F</span><span class=\"token\" style=\"color:#393A34\">.</span><span>log_softmax</span><span class=\"token\" style=\"color:#393A34\">(</span><span>model_output</span><span class=\"token\" style=\"color:#393A34\">,</span><span> dim </span><span class=\"token\" style=\"color:#393A34\">=</span><span> </span><span class=\"token\" style=\"color:#36acaa\">1</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span><span>loss </span><span class=\"token\" style=\"color:#393A34\">=</span><span> F</span><span class=\"token\" style=\"color:#393A34\">.</span><span>nll_loss</span><span class=\"token\" style=\"color:#393A34\">(</span><span>lsm</span><span class=\"token\" style=\"color:#393A34\">,</span><span> targets</span><span class=\"token\" style=\"color:#393A34\">)</span><span>\n</span>loss\n</code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor(1.0314)</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>PyTorch also has a cross-entropy loss that can be used directly on raw model outputs:</p></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-gzVnrw iiEGBE input-container\"><pre class=\"sc-bZQynM dQAqiJ input\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:10px 0px 10px 10px;margin:0px;overflow:auto;border:none;background-color:var(--cm-background, #fafafa)\"><code class=\"language-python\" style=\"color:#393A34;font-family:&quot;Consolas&quot;, &quot;Bitstream Vera Sans Mono&quot;, &quot;Courier New&quot;, Courier, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:.9em;line-height:1.2em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none\"><span>F</span><span class=\"token\" style=\"color:#393A34\">.</span><span>cross_entropy</span><span class=\"token\" style=\"color:#393A34\">(</span><span>model_output</span><span class=\"token\" style=\"color:#393A34\">,</span><span> targets</span><span class=\"token\" style=\"color:#393A34\">)</span></code></pre></div><div class=\"sc-htoDjs fWFfJb nteract-outputs\" style=\"max-height:100%\"><div class=\"cell_display\" style=\"max-height:100%;overflow-y:auto\"><pre><code><span>tensor(1.0314)</span></code></pre></div></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"><p>The relationship between these three approaches can be summarized as follows:</p>\n<p><img src=\"/sm.png\"/></p>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb\">Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD (Fastbook) - Ch. 5</a></li>\n<li><a href=\"https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\">Understanding softmax and the negative log-likelihood</a></li>\n</ul></div></div><div class=\"sc-bxivhb HLmih cell\" style=\"box-shadow:none\"><div class=\"sc-iwsKbI exQuEV markdown\"></div></div></div></div>","json":{"metadata":{"title":"Understanding Cross-Entropy Loss","date":"2021-07-18"}}}},"pageContext":{"id":"e722cd17-703d-538d-a3e5-32b131142ab9 >>> JupyterNotebook"}},"staticQueryHashes":["3069025275","63159454"]}